"""
Prompt Engineering Agent (üìù) - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è AI Coach

–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞:
- –ê–Ω–∞–ª–∏–∑ knowledge base –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
- –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
- A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import json

from agents.base_agent import BaseAgent
from services.supabase_client import SupabaseClient, PromptVersion


@dataclass
class KnowledgeSummary:
    """–°–≤–æ–¥–∫–∞ –∑–Ω–∞–Ω–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
    category: str
    total_claims: int
    avg_evidence_level: float
    avg_confidence: float
    top_claims: List[Dict[str, Any]]
    conflicting_areas: List[str]
    knowledge_gaps: List[str]


class PromptEngineeringAgent(BaseAgent):
    """
    Agent –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:
    - strength_training (—Å–∏–ª–æ–≤—ã–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏)
    - hypertrophy (–≥–∏–ø–µ—Ä—Ç—Ä–æ—Ñ–∏—è)
    - nutrition (–ø–∏—Ç–∞–Ω–∏–µ)
    - recovery (–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ)
    - cardio (–∫–∞—Ä–¥–∏–æ)
    - general (–æ–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏)
    """
    
    # –®–∞–±–ª–æ–Ω—ã –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    PROMPT_TEMPLATES = {
        'strength_training': """You are an expert strength training coach with deep knowledge of exercise science.

Your responses must be based on the following scientific evidence:

{evidence_section}

Guidelines:
1. Always cite the evidence level (1-5) for each claim
2. Distinguish between established facts and emerging research
3. Acknowledge when evidence is conflicting or limited
4. Provide practical, actionable advice
5. Consider individual differences (training age, genetics, injury history)

When evidence is insufficient, say so clearly and explain why.
""",
        'hypertrophy': """You are an expert in muscle hypertrophy and body composition.

Scientific foundation:
{evidence_section}

Response guidelines:
1. Reference specific studies when making claims
2. Explain mechanisms (mTOR, muscle protein synthesis, etc.)
3. Distinguish between trained and untrained individuals
4. Address common myths with evidence
5. Provide periodization recommendations
""",
        'nutrition': """You are a sports nutrition specialist.

Evidence base:
{evidence_section}

Key principles:
1. Base recommendations on peer-reviewed research
2. Consider total caloric context
3. Address nutrient timing when relevant
4. Distinguish between optimal and adequate intake
5. Note individual variability in response
""",
        'recovery': """You are a recovery and regeneration specialist.

Scientific basis:
{evidence_section}

Approach:
1. Emphasize evidence-based recovery modalities
2. Distinguish between active and passive recovery
3. Address sleep, stress, and lifestyle factors
4. Consider training load context
5. Acknowledge limitations in recovery research
""",
        'cardio': """You are a cardiovascular training specialist.

Evidence base:
{evidence_section}

Guidelines:
1. Reference heart rate zones and training intensities
2. Distinguish between aerobic and anaerobic training
3. Consider individual fitness levels
4. Address VO2max and endurance adaptations
5. Provide progressive overload recommendations
""",
        'general': """You are an AI fitness coach powered by scientific research.

Current knowledge base:
{evidence_section}

Core principles:
1. Prioritize safety and long-term health
2. Base recommendations on scientific consensus
3. Acknowledge uncertainty when appropriate
4. Encourage progressive overload
5. Emphasize consistency over perfection
"""
    }
    
    def __init__(
        self,
        supabase: SupabaseClient,
        llm_service: Optional[Any] = None,
        categories: Optional[List[str]] = None
    ):
        super().__init__(name="PromptEngineeringAgent", supabase=supabase)
        self.llm = llm_service
        self.categories = categories or list(self.PROMPT_TEMPLATES.keys())
        self.stats['prompts_generated'] = 0
        self.stats['prompts_activated'] = 0
    
    async def process(self) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç KB –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç/–æ–±–Ω–æ–≤–ª—è–µ—Ç –ø—Ä–æ–º–ø—Ç—ã.
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–æ–≤
        """
        results = {
            'categories_processed': 0,
            'prompts_generated': 0,
            'prompts_activated': 0,
            'errors': []
        }
        
        for category in self.categories:
            try:
                # 1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º knowledge base
                summary = await self._analyze_knowledge(category)
                
                # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
                if await self._should_update_prompt(category, summary):
                    # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
                    prompt = await self._generate_prompt(category, summary)
                    
                    # 4. –í–∞–ª–∏–¥–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
                    if await self._validate_prompt(prompt):
                        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Ä—Å–∏—é
                        version = await self._save_prompt_version(category, prompt, summary)
                        
                        # 6. –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                        if await self._should_activate(version):
                            await self._activate_prompt(version)
                            results['prompts_activated'] += 1
                        
                        results['prompts_generated'] += 1
                
                results['categories_processed'] += 1
                
            except Exception as e:
                self.logger.error(f"Error processing category {category}: {e}")
                results['errors'].append({'category': category, 'error': str(e)})
        
        self.stats['prompts_generated'] += results['prompts_generated']
        self.stats['prompts_activated'] += results['prompts_activated']
        
        return results
    
    async def _analyze_knowledge(self, category: str) -> KnowledgeSummary:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç knowledge base –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
        
        Args:
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∑–Ω–∞–Ω–∏–π
            
        Returns:
            –°–≤–æ–¥–∫–∞ –∑–Ω–∞–Ω–∏–π
        """
        # –ü–æ–ª—É—á–∞–µ–º claims –∏–∑ –ë–î
        claims = await self.supabase.get_claims_by_category_with_filters(
            category=category,
            min_evidence_level=2,
            min_confidence=0.7,
            limit=50
        )
        
        if not claims:
            return KnowledgeSummary(
                category=category,
                total_claims=0,
                avg_evidence_level=0,
                avg_confidence=0,
                top_claims=[],
                conflicting_areas=[],
                knowledge_gaps=[]
            )
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        evidence_levels = [c.evidence_level for c in claims]
        confidences = [c.confidence_score for c in claims]
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø claims
        top_claims = sorted(
            claims,
            key=lambda c: (c.evidence_level, c.confidence_score),
            reverse=True
        )[:10]
        
        # –ò—â–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã
        conflicting = await self._find_conflicting_claims(category, claims)
        
        # –ò—â–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –∑–Ω–∞–Ω–∏—è—Ö
        gaps = self._identify_knowledge_gaps(category, claims)
        
        return KnowledgeSummary(
            category=category,
            total_claims=len(claims),
            avg_evidence_level=sum(evidence_levels) / len(evidence_levels),
            avg_confidence=sum(confidences) / len(confidences),
            top_claims=[self._claim_to_dict(c) for c in top_claims],
            conflicting_areas=conflicting,
            knowledge_gaps=gaps
        )
    
    async def _should_update_prompt(
        self,
        category: str,
        summary: KnowledgeSummary
    ) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å –ø—Ä–æ–º–ø—Ç.
        
        Args:
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è
            summary: –¢–µ–∫—É—â–∞—è —Å–≤–æ–¥–∫–∞ –∑–Ω–∞–Ω–∏–π
            
        Returns:
            True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        """
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –∞–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        current = await self.supabase.get_active_prompt(category)
        
        if not current:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        snapshot = current.knowledge_snapshot
        
        # –ú–Ω–æ–≥–æ –Ω–æ–≤—ã—Ö claims
        if summary.total_claims > snapshot.get('total_claims', 0) * 1.2:
            return True
        
        # –ò–∑–º–µ–Ω–∏–ª–æ—Å—å —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ evidence
        if abs(summary.avg_evidence_level - snapshot.get('avg_evidence_level', 0)) > 0.5:
            return True
        
        # –ü–æ—è–≤–∏–ª–∏—Å—å –Ω–æ–≤—ã–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã
        if len(summary.conflicting_areas) > len(snapshot.get('conflicting_areas', [])):
            return True
        
        # –ü—Ä–æ–º–ø—Ç —Å—Ç–∞—Ä—ã–π (–±–æ–ª—å—à–µ –Ω–µ–¥–µ–ª–∏)
        if current.created_at:
            age = datetime.now() - current.created_at
            if age.days > 7:
                return True
        
        return False
    
    async def _generate_prompt(
        self,
        category: str,
        summary: KnowledgeSummary
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç.
        
        Args:
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è
            summary: –°–≤–æ–¥–∫–∞ –∑–Ω–∞–Ω–∏–π
            
        Returns:
            –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–µ–∫—Ü–∏—é —Å evidence
        evidence_section = self._format_evidence_section(summary)
        
        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–π —à–∞–±–ª–æ–Ω
        template = self.PROMPT_TEMPLATES.get(
            category,
            self.PROMPT_TEMPLATES['general']
        )
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º evidence
        prompt = template.format(evidence_section=evidence_section)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞—Ö –µ—Å–ª–∏ –µ—Å—Ç—å
        if summary.conflicting_areas:
            conflict_section = self._format_conflict_section(summary.conflicting_areas)
            prompt += f"\n\nAreas of Active Research/Debate:\n{conflict_section}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –∑–Ω–∞–Ω–∏—è—Ö
        if summary.knowledge_gaps:
            gaps_section = self._format_gaps_section(summary.knowledge_gaps)
            prompt += f"\n\nCurrent Knowledge Limitations:\n{gaps_section}"
        
        return prompt
    
    def _format_evidence_section(self, summary: KnowledgeSummary) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–µ–∫—Ü–∏—é —Å evidence."""
        lines = [
            f"Total scientific claims: {summary.total_claims}",
            f"Average evidence level: {summary.avg_evidence_level:.1f}/5",
            f"Average confidence: {summary.avg_confidence:.1%}",
            "",
            "Key findings (highest evidence):",
        ]
        
        for i, claim in enumerate(summary.top_claims[:5], 1):
            lines.append(
                f"{i}. [{claim['evidence_level']}/5] {claim['claim']} "
                f"(confidence: {claim['confidence']:.0%})"
            )
        
        return '\n'.join(lines)
    
    async def _validate_prompt(self, prompt: str) -> bool:
        """
        –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            
        Returns:
            True –µ—Å–ª–∏ –ø—Ä–æ–º–ø—Ç –≤–∞–ª–∏–¥–µ–Ω
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É
        if len(prompt) < 100:
            self.logger.warning("Prompt too short")
            return False
        
        if len(prompt) > 8000:
            self.logger.warning("Prompt too long")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–µ–∫—Ü–∏–π
        required_sections = [
            'evidence',
            'scientific',
        ]
        
        prompt_lower = prompt.lower()
        for section in required_sections:
            if section not in prompt_lower:
                self.logger.warning(f"Missing section: {section}")
                return False
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å LLM service, –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
        if self.llm:
            # TODO: –î–æ–±–∞–≤–∏—Ç—å LLM-based –≤–∞–ª–∏–¥–∞—Ü–∏—é
            pass
        
        return True
    
    async def _save_prompt_version(
        self,
        category: str,
        prompt: str,
        summary: KnowledgeSummary
    ) -> PromptVersion:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é –ø—Ä–æ–º–ø—Ç–∞."""
        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é –≤–µ—Ä—Å–∏—é
        latest = await self.supabase.get_latest_prompt_version(category)
        version_num = (latest.version + 1) if latest else 1
        
        prompt_version = PromptVersion(
            id=None,
            category=category,
            prompt_text=prompt,
            version=version_num,
            knowledge_snapshot={
                'total_claims': summary.total_claims,
                'avg_evidence_level': summary.avg_evidence_level,
                'avg_confidence': summary.avg_confidence,
                'conflicting_areas': summary.conflicting_areas,
                'generated_at': datetime.now().isoformat()
            },
            performance_score=None,
            is_active=False,
            created_at=None,
            metadata={
                'generator_version': '1.0',
                'template_used': category
            }
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        saved = await self.supabase.save_prompt_version(prompt_version)
        return saved
    
    async def _should_activate(self, version: PromptVersion) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é."""
        # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è –≤–µ—Ä—Å–∏—è - –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º
        if version.version == 1:
            return True
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ - –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º
        current = await self.supabase.get_active_prompt(version.category)
        if not current:
            return True
        
        # TODO: –î–æ–±–∞–≤–∏—Ç—å A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–∏–∫—É
        # –ü–æ–∫–∞ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –µ—Å–ª–∏ –≤–µ—Ä—Å–∏—è –Ω–æ–≤–∞—è
        return version.version > current.version
    
    async def _activate_prompt(self, version: PromptVersion):
        """–ê–∫—Ç–∏–≤–∏—Ä—É–µ—Ç –≤–µ—Ä—Å–∏—é –ø—Ä–æ–º–ø—Ç–∞."""
        await self.supabase.activate_prompt_version(version.id)
        self.logger.info(f"Activated prompt version {version.version} for {version.category}")
    
    async def _find_conflicting_claims(
        self,
        category: str,
        claims: List[Any]
    ) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–±–ª–∞—Å—Ç–∏ —Å –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–º–∏ claims."""
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å semantic similarity –ø–æ–∏—Å–∫
        # –ü–æ–∫–∞ –∑–∞–≥–ª—É—à–∫–∞
        return []
    
    def _identify_knowledge_gaps(
        self,
        category: str,
        claims: List[Any]
    ) -> List[str]:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤ –∑–Ω–∞–Ω–∏—è—Ö."""
        gaps = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ claims
        if len(claims) < 10:
            gaps.append(f"Limited research available ({len(claims)} claims)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å evidence
        avg_evidence = sum(c.evidence_level for c in claims) / len(claims) if claims else 0
        if avg_evidence < 3:
            gaps.append("Most evidence is from lower-quality studies")
        
        return gaps
    
    def _claim_to_dict(self, claim: Any) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç claim –≤ —Å–ª–æ–≤–∞—Ä—å."""
        return {
            'id': claim.id,
            'claim': claim.claim,
            'evidence_level': claim.evidence_level,
            'confidence': claim.confidence_score,
            'category': claim.category
        }
    
    def _format_conflict_section(self, conflicts: List[str]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–µ–∫—Ü–∏—é –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤."""
        return '\n'.join(f"- {c}" for c in conflicts)
    
    def _format_gaps_section(self, gaps: List[str]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–µ–∫—Ü–∏—é –ø—Ä–æ–±–µ–ª–æ–≤."""
        return '\n'.join(f"- {g}" for g in gaps)
